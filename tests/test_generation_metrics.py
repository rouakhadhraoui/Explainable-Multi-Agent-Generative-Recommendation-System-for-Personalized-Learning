# tests/test_generation_metrics.py
"""
Tests pour les m√©triques de g√©n√©ration de contenu

V√©rifie que les m√©triques ROUGE, BERTScore, etc. fonctionnent
"""

import sys
sys.path.append('..')

from evaluation.generation_metrics import GenerationMetrics, print_generation_metrics_report


def test_rouge_1():
    """Test du ROUGE-1"""
    print("\n" + "="*70)
    print("TEST 1 : ROUGE-1 (Unigrammes)")
    print("="*70)
    
    metrics = GenerationMetrics()
    
    # Cas 1 : Textes identiques
    generated = "Python is a programming language"
    reference = "Python is a programming language"
    
    rouge1 = metrics.rouge_n(generated, reference, n=1)
    print(f"\nüìä Cas 1 : Textes identiques")
    print(f"  G√©n√©r√©     : {generated}")
    print(f"  R√©f√©rence  : {reference}")
    print(f"  Precision  : {rouge1['precision']:.4f}")
    print(f"  Recall     : {rouge1['recall']:.4f}")
    print(f"  F1         : {rouge1['f1']:.4f}")
    
    assert rouge1['f1'] == 1.0, "‚ùå F1 devrait √™tre 1.0 pour textes identiques"
    
    # Cas 2 : Chevauchement partiel
    generated2 = "Python is a great programming language"
    reference2 = "Python is a programming language"
    
    rouge1_2 = metrics.rouge_n(generated2, reference2, n=1)
    print(f"\nüìä Cas 2 : Chevauchement partiel")
    print(f"  G√©n√©r√©     : {generated2}")
    print(f"  R√©f√©rence  : {reference2}")
    print(f"  Precision  : {rouge1_2['precision']:.4f}")
    print(f"  Recall     : {rouge1_2['recall']:.4f}")
    print(f"  F1         : {rouge1_2['f1']:.4f}")
    
    assert 0 < rouge1_2['f1'] < 1, "‚ùå F1 devrait √™tre entre 0 et 1"
    
    # Cas 3 : Aucun chevauchement
    generated3 = "Machine learning algorithms"
    reference3 = "Python programming language"
    
    rouge1_3 = metrics.rouge_n(generated3, reference3, n=1)
    print(f"\nüìä Cas 3 : Aucun chevauchement")
    print(f"  G√©n√©r√©     : {generated3}")
    print(f"  R√©f√©rence  : {reference3}")
    print(f"  F1         : {rouge1_3['f1']:.4f}")
    
    assert rouge1_3['f1'] == 0.0, "‚ùå F1 devrait √™tre 0.0"
    
    print("\n‚úÖ TEST 1 R√âUSSI - ROUGE-1 correctement impl√©ment√©")


def test_rouge_2():
    """Test du ROUGE-2"""
    print("\n" + "="*70)
    print("TEST 2 : ROUGE-2 (Bigrammes)")
    print("="*70)
    
    metrics = GenerationMetrics()
    
    generated = "Python is a great programming language for beginners"
    reference = "Python is a programming language for beginners"
    
    rouge2 = metrics.rouge_n(generated, reference, n=2)
    print(f"\nüìä Test ROUGE-2")
    print(f"  G√©n√©r√©     : {generated}")
    print(f"  R√©f√©rence  : {reference}")
    print(f"  Precision  : {rouge2['precision']:.4f}")
    print(f"  Recall     : {rouge2['recall']:.4f}")
    print(f"  F1         : {rouge2['f1']:.4f}")
    
    assert 0 <= rouge2['f1'] <= 1, "‚ùå F1 devrait √™tre entre 0 et 1"
    
    print("\n‚úÖ TEST 2 R√âUSSI - ROUGE-2 correctement impl√©ment√©")


def test_rouge_l():
    """Test du ROUGE-L"""
    print("\n" + "="*70)
    print("TEST 3 : ROUGE-L (Longest Common Subsequence)")
    print("="*70)
    
    metrics = GenerationMetrics()
    
    generated = "Python is widely used for data science"
    reference = "Python is used for data analysis and science"
    
    rougeL = metrics.rouge_l(generated, reference)
    print(f"\nüìä Test ROUGE-L")
    print(f"  G√©n√©r√©     : {generated}")
    print(f"  R√©f√©rence  : {reference}")
    print(f"  Precision  : {rougeL['precision']:.4f}")
    print(f"  Recall     : {rougeL['recall']:.4f}")
    print(f"  F1         : {rougeL['f1']:.4f}")
    
    assert 0 <= rougeL['f1'] <= 1, "‚ùå F1 devrait √™tre entre 0 et 1"
    
    print("\n‚úÖ TEST 3 R√âUSSI - ROUGE-L correctement impl√©ment√©")


def test_bertscore():
    """Test du BERTScore"""
    print("\n" + "="*70)
    print("TEST 4 : BERTScore (Similarit√© S√©mantique)")
    print("="*70)
    
    metrics = GenerationMetrics()
    
    # Phrases s√©mantiquement similaires
    generated = "Python is a popular programming language"
    reference = "Python is a widely-used coding language"
    
    print(f"\nüîÑ Chargement du mod√®le d'embeddings...")
    bertscore = metrics.bertscore_simple(generated, reference)
    
    print(f"\nüìä Test BERTScore")
    print(f"  G√©n√©r√©     : {generated}")
    print(f"  R√©f√©rence  : {reference}")
    print(f"  BERTScore  : {bertscore:.4f}")
    
    assert 0 <= bertscore <= 1, "‚ùå BERTScore devrait √™tre entre 0 et 1"
    assert bertscore > 0.5, "‚ùå Phrases similaires devraient avoir BERTScore > 0.5"
    
    print("\n‚úÖ TEST 4 R√âUSSI - BERTScore correctement impl√©ment√©")


def test_readability():
    """Test des m√©triques de lisibilit√©"""
    print("\n" + "="*70)
    print("TEST 5 : M√©triques de Lisibilit√©")
    print("="*70)
    
    metrics = GenerationMetrics()
    
    text = "Python is a high-level programming language. It is easy to learn and widely used."
    
    readability = metrics.readability_score(text)
    
    print(f"\nüìñ Texte analys√©:")
    print(f"  {text}")
    
    print(f"\nüìä M√©triques de lisibilit√©:")
    print(f"  Longueur moy. mots : {readability['avg_word_length']:.2f}")
    print(f"  Total mots         : {readability['total_words']}")
    print(f"  Mots uniques       : {readability['unique_words']}")
    print(f"  Diversit√© lexicale : {readability['lexical_diversity']:.4f}")
    
    assert readability['total_words'] > 0, "‚ùå Devrait avoir des mots"
    assert 0 <= readability['lexical_diversity'] <= 1, "‚ùå Diversit√© entre 0 et 1"
    
    print("\n‚úÖ TEST 5 R√âUSSI - M√©triques de lisibilit√© OK")


def test_coherence():
    """Test de la coh√©rence"""
    print("\n" + "="*70)
    print("TEST 6 : Score de Coh√©rence")
    print("="*70)
    
    metrics = GenerationMetrics()
    
    # Texte avec connecteurs
    text1 = "Python is popular. However, it can be slow. Therefore, optimization is important."
    coherence1 = metrics.coherence_score(text1)
    
    print(f"\nüìä Texte avec connecteurs:")
    print(f"  {text1}")
    print(f"  Coh√©rence : {coherence1:.4f}")
    
    # Texte sans connecteurs
    text2 = "Python is popular. It can be slow. Optimization is important."
    coherence2 = metrics.coherence_score(text2)
    
    print(f"\nüìä Texte sans connecteurs:")
    print(f"  {text2}")
    print(f"  Coh√©rence : {coherence2:.4f}")
    
    assert coherence1 > coherence2, "‚ùå Texte avec connecteurs devrait avoir meilleure coh√©rence"
    
    print("\n‚úÖ TEST 6 R√âUSSI - Score de coh√©rence OK")


def test_evaluate_all():
    """Test de l'√©valuation compl√®te"""
    print("\n" + "="*70)
    print("TEST 7 : √âvaluation Compl√®te")
    print("="*70)
    
    metrics = GenerationMetrics()
    
    generated = """Python is a high-level programming language. 
    It is widely used for data science and machine learning. 
    Python has a simple syntax that makes it easy to learn."""
    
    reference = """Python is a popular programming language. 
    It is commonly used for data analysis and AI. 
    Python has clear syntax that makes it beginner-friendly."""
    
    print(f"\nüìù Texte g√©n√©r√©:")
    print(f"  {generated[:80]}...")
    
    print(f"\nüìù Texte de r√©f√©rence:")
    print(f"  {reference[:80]}...")
    
    results = metrics.evaluate_all(generated, reference)
    
    print_generation_metrics_report(results, title="R√©sultats d'√âvaluation")
    
    # V√©rifications
    assert 'ROUGE-1_f1' in results, "‚ùå ROUGE-1 F1 manquant"
    assert 'BERTScore' in results, "‚ùå BERTScore manquant"
    assert 'coherence' in results, "‚ùå Coh√©rence manquante"
    
    print("‚úÖ TEST 7 R√âUSSI - √âvaluation compl√®te fonctionnelle")


def test_batch_evaluation():
    """Test de l'√©valuation en batch"""
    print("\n" + "="*70)
    print("TEST 8 : √âvaluation en BATCH")
    print("="*70)
    
    metrics = GenerationMetrics()
    
    batch = [
        ("Python is great", "Python is excellent"),
        ("Java is popular", "Java is widely used"),
        ("C++ is fast", "C++ is efficient")
    ]
    
    avg_results = metrics.evaluate_batch(batch)
    
    print_generation_metrics_report(avg_results, title="R√©sultats Moyens (3 textes)")
    
    # V√©rifications
    assert 'ROUGE-1_f1_std' in avg_results, "‚ùå √âcart-type manquant"
    assert 'BERTScore_std' in avg_results, "‚ùå √âcart-type BERTScore manquant"
    
    print("‚úÖ TEST 8 R√âUSSI - √âvaluation batch fonctionnelle")


def run_all_tests():
    """Ex√©cuter tous les tests"""
    print("\n" + "#"*70)
    print("# SUITE DE TESTS - M√âTRIQUES DE G√âN√âRATION")
    print("#"*70)
    
    try:
        test_rouge_1()
        test_rouge_2()
        test_rouge_l()
        test_bertscore()
        test_readability()
        test_coherence()
        test_evaluate_all()
        test_batch_evaluation()
        
        print("\n" + "="*70)
        print("üéâ TOUS LES TESTS DES M√âTRIQUES DE G√âN√âRATION R√âUSSIS !")
        print("="*70)
        print("\n‚úÖ ROUGE, BERTScore et autres m√©triques impl√©ment√©es")
        print("‚úÖ Pr√™t pour √©valuer la qualit√© du contenu g√©n√©r√©\n")
        
    except AssertionError as e:
        print(f"\n‚ùå √âCHEC DU TEST: {e}")
    except Exception as e:
        print(f"\n‚ùå ERREUR INATTENDUE: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    run_all_tests()