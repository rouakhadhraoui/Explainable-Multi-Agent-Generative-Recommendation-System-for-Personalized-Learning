# evaluation/system_evaluation.py
"""
√âvaluation Compl√®te du Syst√®me Multi-Agents

Ce module √©value le syst√®me complet sur le dataset OULAD avec toutes les m√©triques :
- M√©triques de recommandation (NDCG, MRR, Recall@K)
- M√©triques de g√©n√©ration (ROUGE, BERTScore)
- M√©triques XAI (Faithfulness, Plausibility, Trust Score)

Les r√©sultats sont sauvegard√©s pour publication scientifique.
"""

import json
import os
from datetime import datetime
from typing import Dict, List, Tuple
import numpy as np

from memory.blackboard import Blackboard
from orchestrator.orchestrator import Orchestrator
from utils.oulad_integration import OULADIntegration
from evaluation.recommendation_metrics import RecommendationMetrics, print_metrics_report
from evaluation.generation_metrics import GenerationMetrics, print_generation_metrics_report
from evaluation.xai_metrics import XAIMetrics, print_xai_metrics_report


class SystemEvaluation:
    """
    √âvalue le syst√®me complet avec toutes les m√©triques
    """
    
    def __init__(self, output_dir: str = "evaluation/results"):
        """
        Initialise l'√©valuation
        
        Args:
            output_dir: Dossier pour sauvegarder les r√©sultats
        """
        self.output_dir = output_dir
        os.makedirs(output_dir, exist_ok=True)
        
        # Initialiser les composants
        self.blackboard = Blackboard()
        self.orchestrator = Orchestrator(self.blackboard)
        self.oulad = OULADIntegration(self.blackboard)
        
        # M√©triques
        self.rec_metrics = RecommendationMetrics()
        self.gen_metrics = GenerationMetrics()
        self.xai_metrics = XAIMetrics()
        
        print(f"‚úì System Evaluation initialis√©")
        print(f"  Dossier de sortie: {output_dir}")
    
    def evaluate_recommendations(self, n_users: int = 50) -> Dict:
        """
        √âvalue les recommandations sur N utilisateurs OULAD
        
        Args:
            n_users: Nombre d'utilisateurs √† √©valuer
        
        Returns:
            Dict avec les r√©sultats des m√©triques
        """
        print(f"\n{'='*70}")
        print(f"üìä √âVALUATION DES RECOMMANDATIONS ({n_users} utilisateurs)")
        print(f"{'='*70}")
        
        # Charger les utilisateurs
        students = self.oulad.load_multiple_students(n=n_users)
        
        batch_recommendations = []
        
        for i, student_id in enumerate(students, 1):
            print(f"\n[{i}/{len(students)}] √âvaluation de {student_id}...")
            
            try:
                # Lancer l'analyse compl√®te
                result = self.orchestrator.process_user_request(
                    student_id, 
                    request_type="full_analysis"
                )
                
                if result['overall_status'] != 'completed':
                    print(f"  ‚ö†Ô∏è  Analyse √©chou√©e pour {student_id}")
                    continue
                
                # R√©cup√©rer les recommandations
                recommendations = self.blackboard.read("recommendations", student_id)
                
                if not recommendations:
                    print(f"  ‚ö†Ô∏è  Pas de recommandations pour {student_id}")
                    continue
                
                # Extraire les IDs recommand√©s
                recommended_ids = [
                    rec['resource_id'] 
                    for rec in recommendations['recommendations']
                ]
                
                # G√©n√©rer un ground truth simple
                # (Dans un vrai syst√®me, utiliser les interactions futures)
                relevant_ids = self._generate_ground_truth(student_id)
                
                batch_recommendations.append((recommended_ids, relevant_ids))
                
                print(f"  ‚úì {len(recommended_ids)} recommandations, {len(relevant_ids)} pertinents")
                
            except Exception as e:
                print(f"  ‚ùå Erreur: {e}")
                continue
        
        # Calculer les m√©triques
        print(f"\nüìä Calcul des m√©triques sur {len(batch_recommendations)} utilisateurs...")
        
        if not batch_recommendations:
            print("‚ö†Ô∏è  Aucune donn√©e √† √©valuer")
            return {}
        
        results = self.rec_metrics.evaluate_batch(
            batch_recommendations,
            k_values=[3, 5, 10]
        )
        
        print_metrics_report(results, title="R√©sultats Recommandations")
        
        return results
    
    def evaluate_generation(self, n_samples: int = 20) -> Dict:
        """
        √âvalue la qualit√© du contenu g√©n√©r√©
        
        Args:
            n_samples: Nombre d'√©chantillons √† √©valuer
        
        Returns:
            Dict avec les r√©sultats des m√©triques
        """
        print(f"\n{'='*70}")
        print(f"üìù √âVALUATION DE LA G√âN√âRATION DE CONTENU ({n_samples} √©chantillons)")
        print(f"{'='*70}")
        
        # R√©cup√©rer les contenus g√©n√©r√©s du cache
        cached_content = self.blackboard.read_section("cached_content")
        
        if not cached_content:
            print("‚ö†Ô∏è  Aucun contenu g√©n√©r√© dans le cache")
            return {}
        
        # Prendre un √©chantillon
        sample_ids = list(cached_content.keys())[:n_samples]
        
        batch_generation = []
        
        for i, content_id in enumerate(sample_ids, 1):
            content = cached_content[content_id]
            
            print(f"\n[{i}/{len(sample_ids)}] √âvaluation de {content_id}...")
            
            # Extraire le texte g√©n√©r√©
            generated_text = self._extract_generated_text(content)
            
            # Cr√©er une r√©f√©rence (dans un vrai syst√®me, utiliser des r√©f√©rences humaines)
            reference_text = self._create_reference_text(content)
            
            if generated_text and reference_text:
                batch_generation.append((generated_text, reference_text))
                print(f"  ‚úì Texte: {len(generated_text)} caract√®res")
        
        # Calculer les m√©triques
        print(f"\nüìä Calcul des m√©triques sur {len(batch_generation)} textes...")
        
        if not batch_generation:
            print("‚ö†Ô∏è  Aucune donn√©e √† √©valuer")
            return {}
        
        results = self.gen_metrics.evaluate_batch(batch_generation)
        
        print_generation_metrics_report(results, title="R√©sultats G√©n√©ration")
        
        return results
    
    def evaluate_xai(self, n_users: int = 30) -> Dict:
        """
        √âvalue la qualit√© des explications XAI
        
        Args:
            n_users: Nombre d'utilisateurs √† √©valuer
        
        Returns:
            Dict avec les r√©sultats des m√©triques
        """
        print(f"\n{'='*70}")
        print(f"üîç √âVALUATION DES EXPLICATIONS XAI ({n_users} utilisateurs)")
        print(f"{'='*70}")
        
        # R√©cup√©rer les explications du Blackboard
        explanations_section = self.blackboard.read_section("explanations")
        
        if not explanations_section:
            print("‚ö†Ô∏è  Aucune explication dans le Blackboard")
            return {}
        
        # Prendre un √©chantillon
        sample_ids = list(explanations_section.keys())[:n_users]
        
        batch_explanations = []
        batch_features = []
        batch_importance = []
        
        for i, user_id in enumerate(sample_ids, 1):
            explanation = explanations_section[user_id]
            
            print(f"\n[{i}/{len(sample_ids)}] √âvaluation XAI de {user_id}...")
            
            # R√©cup√©rer le profil pour les features
            profile = self.blackboard.read("profiles", user_id)
            
            if profile:
                actual_features = {
                    "level": profile['level'],
                    "learning_style": profile['learning_style']
                }
                
                feature_importance = {
                    "level": 0.35,
                    "learning_style": 0.25,
                    "interests": 0.20
                }
                
                batch_explanations.append(explanation)
                batch_features.append(actual_features)
                batch_importance.append(feature_importance)
                
                print(f"  ‚úì Explication avec {len(explanation)} sections")
        
        # Calculer les m√©triques
        print(f"\nüìä Calcul des m√©triques XAI sur {len(batch_explanations)} explications...")
        
        if not batch_explanations:
            print("‚ö†Ô∏è  Aucune donn√©e √† √©valuer")
            return {}
        
        results = self.xai_metrics.evaluate_batch(
            batch_explanations,
            batch_features,
            batch_importance
        )
        
        print_xai_metrics_report(results, title="R√©sultats XAI")
        
        return results
    
    def run_complete_evaluation(self, n_users: int = 30) -> Dict:
        """
        Lance une √©valuation compl√®te du syst√®me
        
        Args:
            n_users: Nombre d'utilisateurs √† √©valuer
        
        Returns:
            Dict avec tous les r√©sultats
        """
        print(f"\n" + "#"*70)
        print(f"# √âVALUATION COMPL√àTE DU SYST√àME MULTI-AGENTS")
        print(f"# Dataset: OULAD | Users: {n_users}")
        print(f"#"*70)
        
        start_time = datetime.now()
        
        results = {
            "metadata": {
                "evaluation_date": start_time.isoformat(),
                "n_users": n_users,
                "dataset": "OULAD",
                "system_version": "1.0"
            },
            "recommendations": {},
            "generation": {},
            "xai": {}
        }
        
        # 1. √âvaluer les recommandations
        try:
            results["recommendations"] = self.evaluate_recommendations(n_users)
        except Exception as e:
            print(f"\n‚ùå Erreur √©valuation recommandations: {e}")
        
        # 2. √âvaluer la g√©n√©ration
        try:
            results["generation"] = self.evaluate_generation(n_samples=min(20, n_users))
        except Exception as e:
            print(f"\n‚ùå Erreur √©valuation g√©n√©ration: {e}")
        
        # 3. √âvaluer XAI
        try:
            results["xai"] = self.evaluate_xai(n_users)
        except Exception as e:
            print(f"\n‚ùå Erreur √©valuation XAI: {e}")
        
        # Temps d'ex√©cution
        end_time = datetime.now()
        duration = (end_time - start_time).total_seconds()
        results["metadata"]["duration_seconds"] = duration
        
        # Sauvegarder les r√©sultats
        self._save_results(results)
        
        # Afficher le r√©sum√© final
        self._print_final_summary(results)
        
        return results
    
    def _generate_ground_truth(self, student_id: str) -> List[str]:
        """
        G√©n√®re un ground truth simple pour les recommandations
        
        Dans un vrai syst√®me, utiliser les interactions futures de l'√©tudiant
        
        Args:
            student_id: ID de l'√©tudiant
        
        Returns:
            Liste des ressources pertinentes
        """
        # Simul√© : utiliser le parcours planifi√© comme r√©f√©rence
        learning_path = self.blackboard.read("learning_paths", student_id)
        
        if learning_path and 'path' in learning_path:
            return [step['resource_id'] for step in learning_path['path'][:5]]
        
        return []
    
    def _extract_generated_text(self, content: Dict) -> str:
        """
        Extrait le texte g√©n√©r√© d'un contenu
        
        Args:
            content: Contenu g√©n√©r√©
        
        Returns:
            Texte g√©n√©r√©
        """
        if 'content' in content:
            content_data = content['content']
            
            if isinstance(content_data, dict):
                return content_data.get('full_text', '')
        
        return ""
    
    def _create_reference_text(self, content: Dict) -> str:
        """
        Cr√©e un texte de r√©f√©rence simple
        
        Dans un vrai syst√®me, utiliser des r√©f√©rences cr√©√©es par des experts
        
        Args:
            content: Contenu g√©n√©r√©
        
        Returns:
            Texte de r√©f√©rence
        """
        # R√©f√©rence simple bas√©e sur le sujet
        topic = content.get('topic', 'programming')
        level = content.get('level', 'beginner')
        content_type = content.get('type', 'course')
        
        reference = f"This {content_type} covers {topic} concepts at {level} level. "
        reference += f"It provides clear explanations and practical examples for learners."
        
        return reference
    
    def _save_results(self, results: Dict):
        """
        Sauvegarde les r√©sultats en JSON
        
        Args:
            results: Dict des r√©sultats
        """
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        filename = f"evaluation_results_{timestamp}.json"
        filepath = os.path.join(self.output_dir, filename)
        
        with open(filepath, 'w') as f:
            json.dump(results, f, indent=2)
        
        print(f"\nüíæ R√©sultats sauvegard√©s: {filepath}")
    
    def _print_final_summary(self, results: Dict):
        """
        Affiche un r√©sum√© final des r√©sultats
        
        Args:
            results: Dict des r√©sultats
        """
        print(f"\n" + "="*70)
        print(f"üìä R√âSUM√â FINAL DE L'√âVALUATION")
        print(f"="*70)
        
        print(f"\n‚è±Ô∏è  Dur√©e totale: {results['metadata'].get('duration_seconds', 0):.2f} secondes")
        print(f"üë• Utilisateurs √©valu√©s: {results['metadata']['n_users']}")
        
        # Recommandations
        if results['recommendations']:
            print(f"\nüéØ Recommandations:")
            for metric in ['NDCG@5', 'MRR', 'Recall@10']:
                if metric in results['recommendations']:
                    print(f"  ‚Ä¢ {metric:15s} : {results['recommendations'][metric]:.4f}")
        
        # G√©n√©ration
        if results['generation']:
            print(f"\nüìù G√©n√©ration de Contenu:")
            for metric in ['ROUGE-1_f1', 'BERTScore']:
                if metric in results['generation']:
                    print(f"  ‚Ä¢ {metric:15s} : {results['generation'][metric]:.4f}")
        
        # XAI
        if results['xai']:
            print(f"\nüîç Explicabilit√© (XAI):")
            for metric in ['faithfulness', 'plausibility', 'trust_score']:
                if metric in results['xai']:
                    print(f"  ‚Ä¢ {metric:15s} : {results['xai'][metric]:.4f}")
        
        print(f"\n{'='*70}")
        print(f"‚úÖ √âVALUATION COMPL√àTE TERMIN√âE !")
        print(f"{'='*70}\n")


def run_evaluation(n_users: int = 30):
    """
    Point d'entr√©e pour lancer l'√©valuation
    
    Args:
        n_users: Nombre d'utilisateurs √† √©valuer
    """
    evaluator = SystemEvaluation()
    results = evaluator.run_complete_evaluation(n_users=n_users)
    return results


if __name__ == "__main__":
    # Lancer l'√©valuation avec 30 utilisateurs
    run_evaluation(n_users=30)