# evaluation/recommendation_metrics.py
"""
MÃ©triques d'Ã©valuation pour les systÃ¨mes de recommandation

ImplÃ©mente les mÃ©triques standard :
- NDCG@K (Normalized Discounted Cumulative Gain)
- MRR (Mean Reciprocal Rank)
- Recall@K
- Precision@K
- MAP (Mean Average Precision)

Ces mÃ©triques sont utilisÃ©es pour Ã©valuer la qualitÃ© des recommandations
par rapport Ã  un ground truth (vÃ©ritÃ© terrain).
"""

import numpy as np
from typing import List, Dict, Tuple, Optional


class RecommendationMetrics:
    """
    Calcule les mÃ©triques d'Ã©valuation pour les recommandations
    """
    
    @staticmethod
    def ndcg_at_k(recommended: List[str], relevant: List[str], k: int = 10) -> float:
        """
        Calcule le NDCG@K (Normalized Discounted Cumulative Gain)
        
        NDCG mesure la qualitÃ© du ranking en tenant compte de la position.
        Les Ã©lÃ©ments pertinents en haut de la liste ont plus de poids.
        
        Args:
            recommended: Liste des items recommandÃ©s (dans l'ordre)
            relevant: Liste des items pertinents (ground truth)
            k: Nombre de recommandations Ã  considÃ©rer
        
        Returns:
            Score NDCG entre 0 et 1 (1 = parfait)
        """
        # Limiter aux k premiÃ¨res recommandations
        recommended = recommended[:k]
        
        if not recommended or not relevant:
            return 0.0
        
        # DCG (Discounted Cumulative Gain)
        dcg = 0.0
        for i, item in enumerate(recommended):
            if item in relevant:
                # Position i+1 (1-indexed), discount = log2(i+2)
                dcg += 1.0 / np.log2(i + 2)
        
        # IDCG (Ideal DCG) - si tous les items pertinents Ã©taient en tÃªte
        idcg = 0.0
        for i in range(min(len(relevant), k)):
            idcg += 1.0 / np.log2(i + 2)
        
        # NDCG = DCG / IDCG
        if idcg == 0:
            return 0.0
        
        return dcg / idcg
    
    @staticmethod
    def mrr(recommended: List[str], relevant: List[str]) -> float:
        """
        Calcule le MRR (Mean Reciprocal Rank)
        
        MRR mesure la position du premier item pertinent.
        Plus il apparaÃ®t tÃ´t, meilleur est le score.
        
        Args:
            recommended: Liste des items recommandÃ©s (dans l'ordre)
            relevant: Liste des items pertinents (ground truth)
        
        Returns:
            Score MRR entre 0 et 1 (1 = premier item est pertinent)
        """
        if not recommended or not relevant:
            return 0.0
        
        for i, item in enumerate(recommended):
            if item in relevant:
                # Reciprocal du rang (1-indexed)
                return 1.0 / (i + 1)
        
        return 0.0
    
    @staticmethod
    def recall_at_k(recommended: List[str], relevant: List[str], k: int = 10) -> float:
        """
        Calcule le Recall@K
        
        Recall@K = nombre d'items pertinents dans top-K / nombre total d'items pertinents
        
        Args:
            recommended: Liste des items recommandÃ©s (dans l'ordre)
            relevant: Liste des items pertinents (ground truth)
            k: Nombre de recommandations Ã  considÃ©rer
        
        Returns:
            Score Recall entre 0 et 1
        """
        if not relevant:
            return 0.0
        
        # Limiter aux k premiÃ¨res recommandations
        recommended = recommended[:k]
        
        # Compter les items pertinents dans les recommandations
        hits = len(set(recommended) & set(relevant))
        
        return hits / len(relevant)
    
    @staticmethod
    def precision_at_k(recommended: List[str], relevant: List[str], k: int = 10) -> float:
        """
        Calcule la Precision@K
        
        Precision@K = nombre d'items pertinents dans top-K / K
        
        Args:
            recommended: Liste des items recommandÃ©s (dans l'ordre)
            relevant: Liste des items pertinents (ground truth)
            k: Nombre de recommandations Ã  considÃ©rer
        
        Returns:
            Score Precision entre 0 et 1
        """
        if not recommended:
            return 0.0
        
        # Limiter aux k premiÃ¨res recommandations
        recommended = recommended[:k]
        
        # Compter les items pertinents dans les recommandations
        hits = len(set(recommended) & set(relevant))
        
        return hits / k
    
    @staticmethod
    def average_precision(recommended: List[str], relevant: List[str]) -> float:
        """
        Calcule l'Average Precision (AP)
        
        AP prend en compte la prÃ©cision Ã  chaque position oÃ¹ un item pertinent apparaÃ®t.
        
        Args:
            recommended: Liste des items recommandÃ©s (dans l'ordre)
            relevant: Liste des items pertinents (ground truth)
        
        Returns:
            Score AP entre 0 et 1
        """
        if not relevant:
            return 0.0
        
        hits = 0
        sum_precisions = 0.0
        
        for i, item in enumerate(recommended):
            if item in relevant:
                hits += 1
                precision_at_i = hits / (i + 1)
                sum_precisions += precision_at_i
        
        if hits == 0:
            return 0.0
        
        return sum_precisions / len(relevant)
    
    @staticmethod
    def hit_rate_at_k(recommended: List[str], relevant: List[str], k: int = 10) -> float:
        """
        Calcule le Hit Rate@K
        
        Hit Rate@K = 1 si au moins un item pertinent est dans top-K, sinon 0
        
        Args:
            recommended: Liste des items recommandÃ©s (dans l'ordre)
            relevant: Liste des items pertinents (ground truth)
            k: Nombre de recommandations Ã  considÃ©rer
        
        Returns:
            1.0 ou 0.0
        """
        recommended = recommended[:k]
        
        # VÃ©rifier s'il y a au moins un hit
        if set(recommended) & set(relevant):
            return 1.0
        
        return 0.0
    
    @classmethod
    def evaluate_all(cls, recommended: List[str], relevant: List[str], 
                    k_values: List[int] = [5, 10, 20]) -> Dict[str, float]:
        """
        Ã‰value toutes les mÃ©triques pour diffÃ©rentes valeurs de K
        
        Args:
            recommended: Liste des items recommandÃ©s (dans l'ordre)
            relevant: Liste des items pertinents (ground truth)
            k_values: Liste des valeurs de K Ã  Ã©valuer
        
        Returns:
            Dictionnaire contenant toutes les mÃ©triques
        """
        results = {}
        
        # MRR et AP (pas de K)
        results['MRR'] = cls.mrr(recommended, relevant)
        results['MAP'] = cls.average_precision(recommended, relevant)
        
        # MÃ©triques pour chaque K
        for k in k_values:
            results[f'NDCG@{k}'] = cls.ndcg_at_k(recommended, relevant, k)
            results[f'Recall@{k}'] = cls.recall_at_k(recommended, relevant, k)
            results[f'Precision@{k}'] = cls.precision_at_k(recommended, relevant, k)
            results[f'HitRate@{k}'] = cls.hit_rate_at_k(recommended, relevant, k)
        
        return results
    
    @classmethod
    def evaluate_batch(cls, batch_recommendations: List[Tuple[List[str], List[str]]],
                      k_values: List[int] = [5, 10, 20]) -> Dict[str, float]:
        """
        Ã‰value un batch de recommandations et calcule les moyennes
        
        Args:
            batch_recommendations: Liste de tuples (recommended, relevant)
            k_values: Liste des valeurs de K Ã  Ã©valuer
        
        Returns:
            Dictionnaire des mÃ©triques moyennes
        """
        all_results = []
        
        for recommended, relevant in batch_recommendations:
            results = cls.evaluate_all(recommended, relevant, k_values)
            all_results.append(results)
        
        # Calculer les moyennes
        avg_results = {}
        
        if all_results:
            for metric in all_results[0].keys():
                values = [r[metric] for r in all_results]
                avg_results[metric] = np.mean(values)
                avg_results[f'{metric}_std'] = np.std(values)
        
        return avg_results


class GroundTruthGenerator:
    """
    GÃ©nÃ¨re un ground truth pour l'Ã©valuation des recommandations
    
    Dans un contexte e-learning, le ground truth peut Ãªtre :
    - Les ressources que l'Ã©tudiant a effectivement consultÃ©es aprÃ¨s
    - Les ressources sur lesquelles l'Ã©tudiant a eu de bons scores
    - Les ressources recommandÃ©es par des experts
    """
    
    @staticmethod
    def from_future_interactions(past_interactions: List[Dict],
                                 future_interactions: List[Dict],
                                 min_score: float = 70.0) -> List[str]:
        """
        GÃ©nÃ¨re un ground truth basÃ© sur les interactions futures
        
        Les items pertinents sont ceux que l'utilisateur a :
        - ConsultÃ©s dans le futur
        - RÃ©ussi avec un bon score (pour les quiz)
        
        Args:
            past_interactions: Interactions passÃ©es (pour l'entraÃ®nement)
            future_interactions: Interactions futures (ground truth)
            min_score: Score minimum pour considÃ©rer un quiz comme rÃ©ussi
        
        Returns:
            Liste des resource_ids pertinents
        """
        relevant = []
        
        for interaction in future_interactions:
            resource_id = interaction.get('resource_id')
            
            if not resource_id:
                continue
            
            # Si c'est un quiz, vÃ©rifier le score
            if interaction.get('type') == 'quiz':
                score = interaction.get('score', 0)
                if score >= min_score:
                    relevant.append(resource_id)
            else:
                # Pour les autres types, considÃ©rer comme pertinent si consultÃ©
                relevant.append(resource_id)
        
        return list(set(relevant))  # Ã‰liminer les doublons
    
    @staticmethod
    def from_successful_peers(user_profile: Dict,
                             all_users_data: List[Dict]) -> List[str]:
        """
        GÃ©nÃ¨re un ground truth basÃ© sur les utilisateurs similaires qui ont rÃ©ussi
        
        Args:
            user_profile: Profil de l'utilisateur cible
            all_users_data: DonnÃ©es de tous les utilisateurs
        
        Returns:
            Liste des resource_ids pertinents
        """
        # Trouver les utilisateurs similaires
        similar_users = []
        
        for other_user in all_users_data:
            if (other_user.get('level') == user_profile.get('level') and
                other_user.get('learning_style') == user_profile.get('learning_style')):
                similar_users.append(other_user)
        
        # RÃ©cupÃ©rer les ressources sur lesquelles ils ont rÃ©ussi
        relevant = []
        
        for user in similar_users:
            for interaction in user.get('interactions', []):
                if interaction.get('type') == 'quiz' and interaction.get('score', 0) >= 80:
                    relevant.append(interaction.get('resource_id'))
        
        return list(set(relevant))


def print_metrics_report(metrics: Dict[str, float], title: str = "MÃ©triques d'Ã‰valuation"):
    """
    Affiche un rapport formatÃ© des mÃ©triques
    
    Args:
        metrics: Dictionnaire des mÃ©triques
        title: Titre du rapport
    """
    print(f"\n{'='*70}")
    print(f"{title:^70}")
    print(f"{'='*70}")
    
    # Grouper par type de mÃ©trique
    mrr_map = {k: v for k, v in metrics.items() if k in ['MRR', 'MAP']}
    ndcg = {k: v for k, v in metrics.items() if 'NDCG' in k and '_std' not in k}
    recall = {k: v for k, v in metrics.items() if 'Recall' in k and '_std' not in k}
    precision = {k: v for k, v in metrics.items() if 'Precision' in k and '_std' not in k}
    hitrate = {k: v for k, v in metrics.items() if 'HitRate' in k and '_std' not in k}
    
    # Afficher par catÃ©gorie
    if mrr_map:
        print(f"\nðŸ“Š MÃ©triques Globales:")
        for metric, value in mrr_map.items():
            std_key = f'{metric}_std'
            if std_key in metrics:
                print(f"  {metric:20s} : {value:.4f} Â± {metrics[std_key]:.4f}")
            else:
                print(f"  {metric:20s} : {value:.4f}")
    
    if ndcg:
        print(f"\nðŸŽ¯ NDCG (Normalized Discounted Cumulative Gain):")
        for metric, value in sorted(ndcg.items()):
            std_key = f'{metric}_std'
            if std_key in metrics:
                print(f"  {metric:20s} : {value:.4f} Â± {metrics[std_key]:.4f}")
            else:
                print(f"  {metric:20s} : {value:.4f}")
    
    if recall:
        print(f"\nðŸ“ˆ Recall (Couverture):")
        for metric, value in sorted(recall.items()):
            std_key = f'{metric}_std'
            if std_key in metrics:
                print(f"  {metric:20s} : {value:.4f} Â± {metrics[std_key]:.4f}")
            else:
                print(f"  {metric:20s} : {value:.4f}")
    
    if precision:
        print(f"\nðŸŽ¯ Precision (PrÃ©cision):")
        for metric, value in sorted(precision.items()):
            std_key = f'{metric}_std'
            if std_key in metrics:
                print(f"  {metric:20s} : {value:.4f} Â± {metrics[std_key]:.4f}")
            else:
                print(f"  {metric:20s} : {value:.4f}")
    
    if hitrate:
        print(f"\nâœ“ Hit Rate (Taux de succÃ¨s):")
        for metric, value in sorted(hitrate.items()):
            std_key = f'{metric}_std'
            if std_key in metrics:
                print(f"  {metric:20s} : {value:.4f} Â± {metrics[std_key]:.4f}")
            else:
                print(f"  {metric:20s} : {value:.4f}")
    
    print(f"\n{'='*70}\n")